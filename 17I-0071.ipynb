{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled4.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jGST8daViYR6"
      },
      "source": [
        "## **Mubeen Khan**\n",
        "## **i170071@nu.edu.pk**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1u-gfvfKBomg"
      },
      "source": [
        "### **Installing & Importing Liberaries**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8kpD54n0qBB9",
        "outputId": "56a3990e-760d-4a92-e3d9-d222856c81c4"
      },
      "source": [
        "!pip install urduhack"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting urduhack\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/59/04/3393a9626b766cfee3187e9ccfa27e73061c24646d60be22a0652de95b4f/urduhack-1.1.1-py3-none-any.whl (105kB)\n",
            "\u001b[K     |████████████████████████████████| 112kB 6.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: Click~=7.1 in /usr/local/lib/python3.7/dist-packages (from urduhack) (7.1.2)\n",
            "Collecting tensorflow-datasets~=3.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ca/c9/d97bdf931edbae9aebc767633d088bd674136d5fe7587ef693b7cb6a1883/tensorflow_datasets-3.2.1-py3-none-any.whl (3.4MB)\n",
            "\u001b[K     |████████████████████████████████| 3.4MB 9.2MB/s \n",
            "\u001b[?25hCollecting tf2crf\n",
            "  Downloading https://files.pythonhosted.org/packages/a1/00/10d130d7b50b2a83116051d01bd93bc49b391ed7e4be96107a8a93b675a6/tf2crf-0.1.32-py2.py3-none-any.whl\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from urduhack) (2019.12.20)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets~=3.1->urduhack) (4.41.1)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets~=3.1->urduhack) (3.12.4)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets~=3.1->urduhack) (2.23.0)\n",
            "Requirement already satisfied: tensorflow-metadata in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets~=3.1->urduhack) (1.0.0)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets~=3.1->urduhack) (1.1.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets~=3.1->urduhack) (1.12.1)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets~=3.1->urduhack) (0.12.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets~=3.1->urduhack) (1.15.0)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets~=3.1->urduhack) (0.3.3)\n",
            "Requirement already satisfied: promise in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets~=3.1->urduhack) (2.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets~=3.1->urduhack) (1.19.5)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets~=3.1->urduhack) (0.16.0)\n",
            "Requirement already satisfied: attrs>=18.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets~=3.1->urduhack) (21.2.0)\n",
            "Requirement already satisfied: tensorflow>=2.1.0 in /usr/local/lib/python3.7/dist-packages (from tf2crf->urduhack) (2.5.0)\n",
            "Collecting tensorflow-addons>=0.8.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/66/4b/e893d194e626c24b3df2253066aa418f46a432fdb68250cde14bf9bb0700/tensorflow_addons-0.13.0-cp37-cp37m-manylinux2010_x86_64.whl (679kB)\n",
            "\u001b[K     |████████████████████████████████| 686kB 40.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.6.1->tensorflow-datasets~=3.1->urduhack) (56.1.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->tensorflow-datasets~=3.1->urduhack) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->tensorflow-datasets~=3.1->urduhack) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->tensorflow-datasets~=3.1->urduhack) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->tensorflow-datasets~=3.1->urduhack) (3.0.4)\n",
            "Requirement already satisfied: googleapis-common-protos<2,>=1.52.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-metadata->tensorflow-datasets~=3.1->urduhack) (1.53.0)\n",
            "Requirement already satisfied: grpcio~=1.34.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->tf2crf->urduhack) (1.34.1)\n",
            "Requirement already satisfied: keras-nightly~=2.5.0.dev in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->tf2crf->urduhack) (2.5.0.dev2021032900)\n",
            "Requirement already satisfied: h5py~=3.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->tf2crf->urduhack) (3.1.0)\n",
            "Requirement already satisfied: flatbuffers~=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->tf2crf->urduhack) (1.12)\n",
            "Requirement already satisfied: wheel~=0.35 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->tf2crf->urduhack) (0.36.2)\n",
            "Requirement already satisfied: gast==0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->tf2crf->urduhack) (0.4.0)\n",
            "Requirement already satisfied: google-pasta~=0.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->tf2crf->urduhack) (0.2.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.6.0,>=2.5.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->tf2crf->urduhack) (2.5.0)\n",
            "Requirement already satisfied: astunparse~=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->tf2crf->urduhack) (1.6.3)\n",
            "Requirement already satisfied: tensorboard~=2.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->tf2crf->urduhack) (2.5.0)\n",
            "Requirement already satisfied: opt-einsum~=3.3.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->tf2crf->urduhack) (3.3.0)\n",
            "Requirement already satisfied: typing-extensions~=3.7.4 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->tf2crf->urduhack) (3.7.4.3)\n",
            "Requirement already satisfied: keras-preprocessing~=1.1.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->tf2crf->urduhack) (1.1.2)\n",
            "Requirement already satisfied: typeguard>=2.7 in /usr/local/lib/python3.7/dist-packages (from tensorflow-addons>=0.8.2->tf2crf->urduhack) (2.7.1)\n",
            "Requirement already satisfied: cached-property; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from h5py~=3.1.0->tensorflow>=2.1.0->tf2crf->urduhack) (1.5.2)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow>=2.1.0->tf2crf->urduhack) (1.0.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow>=2.1.0->tf2crf->urduhack) (0.4.4)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow>=2.1.0->tf2crf->urduhack) (3.3.4)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow>=2.1.0->tf2crf->urduhack) (1.8.0)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow>=2.1.0->tf2crf->urduhack) (1.30.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow>=2.1.0->tf2crf->urduhack) (0.6.1)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.5->tensorflow>=2.1.0->tf2crf->urduhack) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard~=2.5->tensorflow>=2.1.0->tf2crf->urduhack) (4.0.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow>=2.1.0->tf2crf->urduhack) (4.7.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow>=2.1.0->tf2crf->urduhack) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow>=2.1.0->tf2crf->urduhack) (4.2.2)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.5->tensorflow>=2.1.0->tf2crf->urduhack) (3.1.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard~=2.5->tensorflow>=2.1.0->tf2crf->urduhack) (3.4.1)\n",
            "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.7/dist-packages (from rsa<5,>=3.1.4; python_version >= \"3.6\"->google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow>=2.1.0->tf2crf->urduhack) (0.4.8)\n",
            "Installing collected packages: tensorflow-datasets, tensorflow-addons, tf2crf, urduhack\n",
            "  Found existing installation: tensorflow-datasets 4.0.1\n",
            "    Uninstalling tensorflow-datasets-4.0.1:\n",
            "      Successfully uninstalled tensorflow-datasets-4.0.1\n",
            "Successfully installed tensorflow-addons-0.13.0 tensorflow-datasets-3.2.1 tf2crf-0.1.32 urduhack-1.1.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UDAgaLX49rqk"
      },
      "source": [
        "# importing necessary liberaries\n",
        "import csv\n",
        "from pandas import DataFrame, read_csv\n",
        "import os\n",
        "import urduhack\n",
        "import math\n",
        "from urduhack.preprocessing import remove_punctuation\n",
        "from urduhack.preprocessing import remove_accents\n",
        "from urduhack.preprocessing import normalize_whitespace\n",
        "from urduhack.preprocessing import replace_numbers\n",
        "from urduhack.preprocessing import replace_phone_numbers\n",
        "from collections import Counter"
      ],
      "execution_count": 177,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ROp5HhqgB1Au"
      },
      "source": [
        "### **Mounting Google Drive**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gAvZUN3pFL6J",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9795da61-3a32-48d5-eb16-e7a516aac3b9"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hVBYG0xvB5k4"
      },
      "source": [
        "### **Reading Files**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "40ExYn44zceg"
      },
      "source": [
        "# function to read all files in a particular folder\n",
        "def readAllFiles(folderName):\n",
        "    allFiles = os.listdir(folderName)\n",
        "    text = []\n",
        "\n",
        "    for textFile in allFiles:\n",
        "        if textFile.__contains__(\".txt\"):\n",
        "            string = ''\n",
        "            fileObject = open(folderName + textFile, \"rt\", encoding=\"utf-8\")\n",
        "            while(1):\n",
        "                char = fileObject.read(1)\n",
        "                if not char:\n",
        "                    break\n",
        "                elif char == '\\n':\n",
        "                    char = ' '\n",
        "                string += char\n",
        "            text.append(string)\n",
        "    return text"
      ],
      "execution_count": 207,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rYlCqQ1Cx8MG"
      },
      "source": [
        "# function to read file as tokens list\n",
        "def readFile(fileName):\n",
        "    with open(fileName, 'rt', encoding=\"utf-8\") as f:\n",
        "        reader = csv.reader(f)\n",
        "        words = list(reader)\n",
        "    f.close()\n",
        "    return words"
      ],
      "execution_count": 208,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pNqs321REot5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d9b2104f-508e-4cdd-8b45-91275955e8df"
      },
      "source": [
        "# reading train data files + stopwords\n",
        "fakeText = readAllFiles('/content/drive/My Drive/NLP/A5/Train/Fake/')\n",
        "print(len(fakeText))\n",
        "\n",
        "realText = readAllFiles('/content/drive/My Drive/NLP/A5/Train/Real/')\n",
        "print(len(realText))\n",
        "\n",
        "stopWords = readFile('/content/drive/My Drive/NLP/A5/stopwords-ur.txt')\n",
        "print(stopWords)\n",
        "print(len(stopWords))"
      ],
      "execution_count": 209,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "288\n",
            "350\n",
            "[['آئی'], ['آئے'], ['آج'], ['آخر'], ['آخرکبر'], ['آدهی'], ['آًب'], ['آٹھ'], ['آیب'], ['اة'], ['اخبزت'], ['اختتبم'], ['ادھر'], ['ارد'], ['اردگرد'], ['ارکبى'], ['اش'], ['اضتعوبل'], ['اضتعوبلات'], ['اضطرذ'], ['اضکب'], ['اضکی'], ['اضکے'], ['اطراف'], ['اغیب'], ['افراد'], ['الگ'], ['اور'], ['اوًچب'], ['اوًچبئی'], ['اوًچی'], ['اوًچے'], ['اى'], ['اً'], ['اًذر'], ['اًہیں'], ['اٹھبًب'], ['اپٌب'], ['اپٌے'], ['اچھب'], ['اچھی'], ['اچھے'], ['اکثر'], ['اکٹھب'], ['اکٹھی'], ['اکٹھے'], ['اکیلا'], ['اکیلی'], ['اکیلے'], ['اگرچہ'], ['اہن'], ['ایطے'], ['ایک'], ['ب'], ['ت'], ['تبزٍ'], ['تت'], ['تر'], ['ترتیت'], ['تریي'], ['تعذاد'], ['تن'], ['تو'], ['توبم'], ['توہی'], ['توہیں'], ['تٌہب'], ['تک'], ['تھب'], ['تھوڑا'], ['تھوڑی'], ['تھوڑے'], ['تھی'], ['تھے'], ['تیي'], ['ثب'], ['ثبئیں'], ['ثبترتیت'], ['ثبری'], ['ثبرے'], ['ثبعث'], ['ثبلا'], ['ثبلترتیت'], ['ثبہر'], ['ثدبئے'], ['ثرآں'], ['ثراں'], ['ثرش'], ['ثعذ'], ['ثغیر'], ['ثلٌذ'], ['ثلٌذوثبلا'], ['ثلکہ'], ['ثي'], ['ثٌب'], ['ثٌبرہب'], ['ثٌبرہی'], ['ثٌبرہے'], ['ثٌبًب'], ['ثٌذ'], ['ثٌذکرو'], ['ثٌذکرًب'], ['ثٌذی'], ['ثڑا'], ['ثڑوں'], ['ثڑی'], ['ثڑے'], ['ثھر'], ['ثھرا'], ['ثھراہوا'], ['ثھرپور'], ['ثھی'], ['ثہت'], ['ثہتر'], ['ثہتری'], ['ثہتریي'], ['ثیچ'], ['ج'], ['خب'], ['خبرہب'], ['خبرہی'], ['خبرہے'], ['خبهوظ'], ['خبًب'], ['خبًتب'], ['خبًتی'], ['خبًتے'], ['خبًٌب'], ['خت'], ['ختن'], ['خجکہ'], ['خص'], ['خططرذ'], ['خلذی'], ['خو'], ['خواى'], ['خوًہی'], ['خوکہ'], ['خٌبة'], ['خگہ'], ['خگہوں'], ['خگہیں'], ['خیطب'], ['خیطبکہ'], ['در'], ['درخبت'], ['درخہ'], ['درخے'], ['درزقیقت'], ['درضت'], ['دش'], ['دفعہ'], ['دلچطپ'], ['دلچطپی'], ['دلچطپیبں'], ['دو'], ['دور'], ['دوراى'], ['دوضرا'], ['دوضروں'], ['دوضری'], ['دوضرے'], ['دوًوں'], ['دکھبئیں'], ['دکھبتب'], ['دکھبتی'], ['دکھبتے'], ['دکھبو'], ['دکھبًب'], ['دکھبیب'], ['دی'], ['دیب'], ['دیتب'], ['دیتی'], ['دیتے'], ['دیر'], ['دیٌب'], ['دیکھو'], ['دیکھٌب'], ['دیکھی'], ['دیکھیں'], ['دے'], ['ر'], ['راضتوں'], ['راضتہ'], ['راضتے'], ['رریعہ'], ['رریعے'], ['رکي'], ['رکھ'], ['رکھب'], ['رکھتب'], ['رکھتبہوں'], ['رکھتی'], ['رکھتے'], ['رکھی'], ['رکھے'], ['رہب'], ['رہی'], ['رہے'], ['ز'], ['زبصل'], ['زبضر'], ['زبل'], ['زبلات'], ['زبلیہ'], ['زصوں'], ['زصہ'], ['زصے'], ['زقبئق'], ['زقیتیں'], ['زقیقت'], ['زکن'], ['زکویہ'], ['زیبدٍ'], ['صبف'], ['صسیر'], ['صفر'], ['صورت'], ['صورتسبل'], ['صورتوں'], ['صورتیں'], ['ض'], ['ضبت'], ['ضبتھ'], ['ضبدٍ'], ['ضبرا'], ['ضبرے'], ['ضبل'], ['ضبلوں'], ['ضت'], ['ضرور'], ['ضرورت'], ['ضروری'], ['ضلطلہ'], ['ضوچ'], ['ضوچب'], ['ضوچتب'], ['ضوچتی'], ['ضوچتے'], ['ضوچو'], ['ضوچٌب'], ['ضوچی'], ['ضوچیں'], ['ضکب'], ['ضکتب'], ['ضکتی'], ['ضکتے'], ['ضکٌب'], ['ضکی'], ['ضکے'], ['ضیذھب'], ['ضیذھی'], ['ضیذھے'], ['ضیکٌڈ'], ['ضے'], ['طرف'], ['طریق'], ['طریقوں'], ['طریقہ'], ['طریقے'], ['طور'], ['طورپر'], ['ظبہر'], ['ع'], ['عذد'], ['عظین'], ['علاقوں'], ['علاقہ'], ['علاقے'], ['علاوٍ'], ['عووهی'], ['غبیذ'], ['غخص'], ['غذ'], ['غروع'], ['غروعبت'], ['غے'], ['فرد'], ['فی'], ['ق'], ['قجل'], ['قجیلہ'], ['قطن'], ['لئے'], ['لا'], ['لازهی'], ['لو'], ['لوجب'], ['لوجی'], ['لوجے'], ['لوسبت'], ['لوسہ'], ['لوگ'], ['لوگوں'], ['لڑکپي'], ['لگتب'], ['لگتی'], ['لگتے'], ['لگٌب'], ['لگی'], ['لگیں'], ['لگے'], ['لی'], ['لیب'], ['لیٌب'], ['لیں'], ['لے'], ['ه'], ['هتعلق'], ['هختلف'], ['هسترم'], ['هسترهہ'], ['هسطوش'], ['هسیذ'], ['هطئلہ'], ['هطئلے'], ['هطبئل'], ['هطتعول'], ['هطلق'], ['هعلوم'], ['هػتول'], ['هلا'], ['هوکي'], ['هوکٌبت'], ['هوکٌہ'], ['هٌبضت'], ['هڑا'], ['هڑًب'], ['هڑے'], ['هکول'], ['هگر'], ['هہرثبى'], ['هیرا'], ['هیری'], ['هیرے'], ['هیں'], ['و'], ['وار'], ['والے'], ['وٍ'], ['ًئی'], ['ًئے'], ['ًب'], ['ًبپطٌذ'], ['ًبگسیر'], ['ًطجت'], ['ًقطہ'], ['ًو'], ['ًوخواى'], ['ًکبلٌب'], ['ًکتہ'], ['ًہ'], ['ًہیں'], ['ًیب'], ['ًے'], ['ٓ آش'], ['ٹھیک'], ['پبئے'], ['پبش'], ['پبًب'], ['پبًچ'], ['پر'], ['پراًب'], ['پطٌذ'], ['پل'], ['پورا'], ['پوچھب'], ['پوچھتب'], ['پوچھتی'], ['پوچھتے'], ['پوچھو'], ['پوچھوں'], ['پوچھٌب'], ['پوچھیں'], ['پچھلا'], ['پھر'], ['پہلا'], ['پہلی'], ['پہلےضی'], ['پہلےضے'], ['پہلےضےہی'], ['پیع'], ['چبر'], ['چبہب'], ['چبہٌب'], ['چبہے'], ['چلا'], ['چلو'], ['چلیں'], ['چلے'], ['چکب'], ['چکی'], ['چکیں'], ['چکے'], ['چھوٹب'], ['چھوٹوں'], ['چھوٹی'], ['چھوٹے'], ['چھہ'], ['چیسیں'], ['ڈھوًڈا'], ['ڈھوًڈلیب'], ['ڈھوًڈو'], ['ڈھوًڈًب'], ['ڈھوًڈی'], ['ڈھوًڈیں'], ['ک'], ['کئی'], ['کئے'], ['کب'], ['کبفی'], ['کبم'], ['کت'], ['کجھی'], ['کرا'], ['کرتب'], ['کرتبہوں'], ['کرتی'], ['کرتے'], ['کرتےہو'], ['کررہب'], ['کررہی'], ['کررہے'], ['کرو'], ['کرًب'], ['کریں'], ['کرے'], ['کطی'], ['کل'], ['کن'], ['کوئی'], ['کوتر'], ['کورا'], ['کوروں'], ['کورٍ'], ['کورے'], ['کوطي'], ['کوى'], ['کوًطب'], ['کوًطی'], ['کوًطے'], ['کھولا'], ['کھولو'], ['کھولٌب'], ['کھولی'], ['کھولیں'], ['کھولے'], ['کہ'], ['کہب'], ['کہتب'], ['کہتی'], ['کہتے'], ['کہو'], ['کہوں'], ['کہٌب'], ['کہی'], ['کہیں'], ['کہے'], ['کی'], ['کیب'], ['کیطب'], ['کیطرف'], ['کیطے'], ['کیلئے'], ['کیوًکہ'], ['کیوں'], ['کیے'], ['کے'], ['کےثعذ'], ['کےرریعے'], ['گئی'], ['گئے'], ['گب'], ['گرد'], ['گروٍ'], ['گروپ'], ['گروہوں'], ['گٌتی'], ['گی'], ['گیب'], ['گے'], ['ہر'], ['ہن'], ['ہو'], ['ہوئی'], ['ہوئے'], ['ہوا'], ['ہوبرا'], ['ہوبری'], ['ہوبرے'], ['ہوتب'], ['ہوتی'], ['ہوتے'], ['ہورہب'], ['ہورہی'], ['ہورہے'], ['ہوضکتب'], ['ہوضکتی'], ['ہوضکتے'], ['ہوًب'], ['ہوًی'], ['ہوًے'], ['ہوچکب'], ['ہوچکی'], ['ہوچکے'], ['ہوگئی'], ['ہوگئے'], ['ہوگیب'], ['ہوں'], ['ہی'], ['ہیں'], ['ہے'], ['ی'], ['یقیٌی'], ['یہ'], ['یہبں']]\n",
            "517\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0VnZWNXqSwtz",
        "outputId": "bb51aa6c-0038-4165-9e7c-844eb8f2a6d7"
      },
      "source": [
        "# reading test data files\n",
        "testFakeText = readAllFiles('/content/drive/My Drive/NLP/A5/Test/Fake/')\n",
        "print(len(fakeText))\n",
        "\n",
        "testRealText = readAllFiles('/content/drive/My Drive/NLP/A5/Test/Real/')\n",
        "print(len(realText))"
      ],
      "execution_count": 210,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "288\n",
            "350\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LmMxIyJvJbm_"
      },
      "source": [
        "### **Removing Duplicates for Boolean Na ̈ıve Bayes**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QllExXDaJaf1",
        "outputId": "2643638d-2d89-4ba7-a2a2-02c21d53b12c"
      },
      "source": [
        "# function to remove duplication for each text given to it and returns a string having no duplicate words\n",
        "def removeDuplicates(text):\n",
        "    text = text.split(\" \")  # tokenizing text \n",
        "  \n",
        "    # joining two adjacent elements in iterable way\n",
        "    for i in range(0, len(text)):\n",
        "        text[i] = \"\".join(text[i])\n",
        "          \n",
        "    # creating dict. using counter method\n",
        "    uniqueWords = Counter(text)\n",
        "  \n",
        "    # joining two adjacent elements in iterable way\n",
        "    s = \" \".join(uniqueWords.keys())\n",
        "    return s\n",
        "\n",
        "print(removeDuplicates(\"hello hi hello hello yo yo hi\"), end='')"
      ],
      "execution_count": 211,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "hello hi yo"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "--b-5oL6KHNr"
      },
      "source": [
        "# removing duplicates from train data and store the results in lists\n",
        "noDupFText = []\n",
        "noDupRText = []\n",
        "\n",
        "for text in fakeText:\n",
        "    noDupFText.append(removeDuplicates(text))\n",
        "\n",
        "for text in realText:\n",
        "    noDupRText.append(removeDuplicates(text))"
      ],
      "execution_count": 212,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pxu62FtRCT9x"
      },
      "source": [
        "### **Removing Stop Words**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QI_7LXvk1mTi",
        "outputId": "b6326edf-14b0-47b7-d31c-c52225ef73a8"
      },
      "source": [
        "# function to remove stop-words from the give list of text\n",
        "def removeStopWords(text, stopWords):\n",
        "    listWSW = []\n",
        "    for t in text:\n",
        "        s = ''\n",
        "        tokens = t.split()\n",
        "        for token in tokens:\n",
        "            if (token not in stopWords):\n",
        "                s += token + ' '\n",
        "        if len(s) > 0:\n",
        "            listWSW.append(s)\n",
        "    return listWSW\n",
        "\n",
        "x = ['hi', 'how', 'are', 'you', 'hello', 'world', 'hi', 'bro']\n",
        "y = ['hi', 'bro']\n",
        "z = removeStopWords(x, y)\n",
        "print(z)"
      ],
      "execution_count": 213,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['how ', 'are ', 'you ', 'hello ', 'world ']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_88WIefSQntC"
      },
      "source": [
        "# removing stop-words from train data with and without duplicates and storing the results\n",
        "\n",
        "fTextWSW = removeStopWords(fakeText, stopWords)\n",
        "rTextWSW = removeStopWords(realText, stopWords)\n",
        "\n",
        "noDupFTextWSW = removeStopWords(noDupFText, stopWords)\n",
        "noDupRTextWSW = removeStopWords(noDupRText, stopWords)"
      ],
      "execution_count": 214,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wQsqgejeCFpi"
      },
      "source": [
        "### **Data Preprocessing (Data Cleaning)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n_Fyrb4wAL8u",
        "outputId": "52205c6f-0510-4ae3-d593-768a7dd752d5"
      },
      "source": [
        "# fuction to remove unnecessary words from text\n",
        "def dataPreprocessing(text):\n",
        "    for i in range(len(text)):\n",
        "        s = ''\n",
        "        for c in text[i]:\n",
        "            if (c.isdigit()):\n",
        "                s += ' '\n",
        "            else:\n",
        "                s += c\n",
        "        text[i] = s\n",
        "        \n",
        "    for i in range(len(text)):\n",
        "        text[i] = normalize_whitespace(remove_punctuation(replace_numbers(remove_accents(text[i]))))\n",
        "\n",
        "text = \" سے   #@!#  230درجات پر مشمل20  فیصد\"\n",
        "l = [text]\n",
        "dataPreprocessing(l)\n",
        "print(l)"
      ],
      "execution_count": 215,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['سے درجات پر مشمل فیصد']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MoCQ4hbIKoE9"
      },
      "source": [
        "# funtion calling\n",
        "\n",
        "dataPreprocessing(fakeText)\n",
        "dataPreprocessing(realText)\n",
        "\n",
        "dataPreprocessing(fTextWSW)\n",
        "dataPreprocessing(rTextWSW)\n",
        "\n",
        "dataPreprocessing(noDupFText)\n",
        "dataPreprocessing(noDupRText)\n",
        "\n",
        "dataPreprocessing(noDupFTextWSW)\n",
        "dataPreprocessing(noDupRTextWSW)\n",
        "\n",
        "dataPreprocessing(testFakeText)\n",
        "dataPreprocessing(testRealText)"
      ],
      "execution_count": 216,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a4hnxkeEbTW_",
        "outputId": "83e69a6b-dcc0-4ddb-8ea2-724a873b9e0c"
      },
      "source": [
        "print (count(fakeText), count(fTextWSW))\n",
        "print (count(realText), count(rTextWSW))\n",
        "print (count(noDupFText), count(noDupFTextWSW))\n",
        "print (count(noDupRText), count(noDupRTextWSW))"
      ],
      "execution_count": 217,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "387913 387913\n",
            "576608 576608\n",
            "219856 219856\n",
            "310372 310372\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a-IoDl7ACNKQ"
      },
      "source": [
        "### **Tokenizing Text**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7KcnwW84D0hB",
        "outputId": "a7d2aa00-e973-498f-ca0e-bbfa4cf39867"
      },
      "source": [
        "# function to tokenize given text\n",
        "def tokenizeText(text):\n",
        "    tokens = []\n",
        "    for i in range(len(text)):\n",
        "        tokens.extend(text[i].split())\n",
        "    return tokens\n",
        "\n",
        "x = [\"hi how are you\", \" hello world\", \"hi bro 1234\"]\n",
        "y = tokenizeText(x)\n",
        "print(y)"
      ],
      "execution_count": 218,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['hi', 'how', 'are', 'you', 'hello', 'world', 'hi', 'bro', '1234']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bn0D0sWWCZUT"
      },
      "source": [
        "### **UniGram Dictionary Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kUzC81-GFTLH",
        "outputId": "2a58d3a8-eabb-463b-a6af-a0a40961fab6"
      },
      "source": [
        "# function to make unigram dict. with freq.\n",
        "def unigramModel(tokens):\n",
        "    dic = {}\n",
        "    for token in tokens:\n",
        "        if token in dic:\n",
        "            dic[token] += 1\n",
        "        else:\n",
        "            dic[token] = 1\n",
        "    return dic\n",
        "\n",
        "x = ['hi', 'how', 'are', 'you', 'hello', 'world', 'hi', 'bro']\n",
        "z = unigramModel(x)\n",
        "print(z)"
      ],
      "execution_count": 219,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'hi': 2, 'how': 1, 'are': 1, 'you': 1, 'hello': 1, 'world': 1, 'bro': 1}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "10x8J8ZKCfcA"
      },
      "source": [
        "### **Extracting Vocabolary**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wCw_QR4WHn5K",
        "outputId": "ee7a260c-2623-4dfe-9f63-c4e984dd4262"
      },
      "source": [
        "# function to extract vocab. - I did it using x = unigramModel() because I calculated freq. which means x have unique keys, thus, less iterations were used\n",
        "def extractVocabolary(text):\n",
        "    vocab = []\n",
        "    for i in range(len(text)):\n",
        "        for token in text[i]:\n",
        "            if token not in vocab:\n",
        "                vocab.append(token)\n",
        "    return vocab\n",
        "\n",
        "x = ['hi','hi','hi','hi', 'how', 'are', 'you', 'hello', 'world', 'hi', 'bro']\n",
        "v = extractVocabolary([x])\n",
        "print(v)"
      ],
      "execution_count": 220,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['hi', 'how', 'are', 'you', 'hello', 'world', 'bro']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R7l3cVOpBb1G"
      },
      "source": [
        "### **Na ̈ıve Bayes Classifier.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9xalHn5DTffk"
      },
      "source": [
        "# NaiveBayesTraining same as given in assignment.pdf\n",
        "# this model takes 2 arguments: C = no. of classes, T = list of texts\n",
        "# for this assignment, C = [0, 1] and T = [fakeText, RealText]\n",
        "# Since the model is generic so I had only to modify T for all for classifiers\n",
        "# classifiers = (Naive Bayes with Stop-Words, Naive Bayes without Stop-Words, Boolean Naive Bayes with Stop-Words, Boolean Naive Bayes without Stop-Words)\n",
        "\n",
        "def NaiveBayesTraining(C, T):\n",
        "    N = len(T[0]) + len(T[1])   # total text - (total files) - (fake, real)\n",
        "\n",
        "    fTokens = tokenizeText(T[0])    # tokenizing fake text\n",
        "    rTokens = tokenizeText(T[1])    # tokenizing real text\n",
        "    tokensList = [fTokens, rTokens] # list of both\n",
        "\n",
        "    V = extractVocabolary(tokensList) # total vocabolary\n",
        "\n",
        "    fDict = unigramModel(fTokens)   # fake unigram model\n",
        "    rDict = unigramModel(rTokens)   # real unigram model\n",
        "    allDicts = [fDict, rDict]   # list of both\n",
        "\n",
        "    prior = {}  # dict. to store priors\n",
        "    condproc = [{},{}]  # list of dicts to store condproc for each class c\n",
        "\n",
        "    for c in C:\n",
        "        Nc = len(T[c])  # length of particular class words\n",
        "        Nw = len(tokensList[c]) # length of all words\n",
        "        prior[c] = Nc/N # calculating prior\n",
        "        docc = allDicts[c]  # dict. of particular class - for freq. purpose\n",
        "        for wi in docc:\n",
        "            condproc[c][wi] = (allDicts[c][wi]+1)/(Nw + len(V)) # calculating conditional probability\n",
        "    return V, prior, condproc"
      ],
      "execution_count": 221,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PG0lwNy_e0jz"
      },
      "source": [
        "C = [0, 1]\n",
        "\n",
        "T1 = [fakeText, realText]\n",
        "T2 = [fTextWSW, rTextWSW]\n",
        "\n",
        "T3 = [noDupFText, noDupRText]\n",
        "T4 = [noDupFTextWSW, noDupRTextWSW]\n",
        "\n",
        "NB = NaiveBayesTraining(C, T1)  # Naive Bayes with Stop-Words\n",
        "NB_WSW = NaiveBayesTraining(C, T2)  # Naive Bayes without Stop-Words\n",
        "\n",
        "BNB = NaiveBayesTraining(C, T3) # Boolean Naive Bayes with Stop-Words\n",
        "BNB_WSW = NaiveBayesTraining(C, T4) # Boolean Naive Bayes without Stop-Words"
      ],
      "execution_count": 222,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uURHMEX8iB_r"
      },
      "source": [
        "### **Performance Evaluation Matrices**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5EEqX__iglDN"
      },
      "source": [
        "# following function will predict whether a text is real or fake. Parameters are the text and the classifier type\n",
        "def predictClassForText(text, classifier):\n",
        "    words = text.split()\n",
        "    checkList = []\n",
        "\n",
        "    fakeProb = math.log2(NB[1][0])  # adding prior fake probability by using log to avoid underflowing\n",
        "    trueProb = math.log2(NB[1][1])  # adding prior real probability by using log to avoid underflowing\n",
        "\n",
        "    if classifier == \"Naive Bayes with Stop-Words\":\n",
        "        for word in words:\n",
        "            if word in NB[2][0]:\n",
        "                fakeProb = math.log2(NB[2][0][word])\n",
        "            if word in NB[2][1]:\n",
        "                trueProb = math.log2(NB[2][1][word])\n",
        "\n",
        "    elif classifier == \"Naive Bayes without Stop-Words\":\n",
        "        for word in words:\n",
        "            if word in NB_WSW[2][0]:\n",
        "                fakeProb = math.log2(NB_WSW[2][0][word])\n",
        "            if word in NB_WSW[2][1]:\n",
        "                trueProb = math.log2(NB_WSW[2][1][word])\n",
        "\n",
        "    elif classifier == \"Boolean Naive Bayes with Stop-Words\":\n",
        "        for word in words:\n",
        "            if word not in checkList:\n",
        "                if word in BNB[2][0]:\n",
        "                    fakeProb = math.log2(BNB[2][0][word])\n",
        "                if word in BNB[2][1]:\n",
        "                    trueProb = math.log2(BNB[2][1][word])\n",
        "                checkList.append(word)\n",
        "\n",
        "    elif classifier == \"Boolean Naive Bayes without Stop-Words\":\n",
        "        for word in words:\n",
        "            if word not in checkList:\n",
        "                if word in BNB_WSW[2][0]:\n",
        "                    fakeProb = math.log2(\n",
        "                        noDupNB_WSW[2][0][word])\n",
        "                if word in BNB_WSW[2][1]:\n",
        "                    trueProb = math.log2(\n",
        "                        noDupNB_WSW[2][1][word])\n",
        "                checkList.append(word)\n",
        "\n",
        "    # returning 0 for fake and 1 for real\n",
        "    if fakeProb < trueProb:\n",
        "        return 1\n",
        "    return 0\n"
      ],
      "execution_count": 223,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GbeOaK0LA32k",
        "outputId": "f41a5acb-733b-4296-d178-7970af043493"
      },
      "source": [
        "# list to contain different classifier names\n",
        "classifiers = ['Naive Bayes with Stop-Words', 'Naive Bayes without Stop-Words', 'Boolean Naive Bayes with Stop-Words', 'Boolean Naive Bayes without Stop-Words']\n",
        "\n",
        "# predicting - returns 0 for fake and 1 for real\n",
        "for classifier in classifiers:\n",
        "    trueList = [] # for real test text - contains 0 if predicted fake, 1 for real\n",
        "    wrongList = [] # for real test text - contains 0 if predicted fake, 1 for real\n",
        "\n",
        "    for text in testFakeText:\n",
        "        wrongList.append(predictClassForText(text, classifier))\n",
        "    for text in testRealText:\n",
        "        trueList.append(predictClassForText(text, classifier))\n",
        "\n",
        "    # displaying matrix using formulas\n",
        "    print(\"-----------------------------------------------------------\\n\")\n",
        "    print(\"Classifier:\\t\", classifier)\n",
        "    accuracy = (trueList.count(1) + wrongList.count(0)) / (len(trueList) + len(wrongList))\n",
        "    print(\"\\nAccuracy:\\t\", accuracy)\n",
        "    recall = trueList.count(1) / len(trueList)\n",
        "    print(\"Recall:\\t\\t\", recall)\n",
        "    precision = trueList.count(1) / (trueList.count(1) + wrongList.count(1))\n",
        "    print(\"Precision:\\t\",precision)\n",
        "    f_1 = (recall * precision * 2) / (precision + recall)\n",
        "    print(\"F1 Measure:\\t\", f_1)\n",
        "    print(\"-----------------------------------------------------------\\n\")"
      ],
      "execution_count": 224,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-----------------------------------------------------------\n",
            "\n",
            "Classifier:\t Naive Bayes with Stop-Words\n",
            "\n",
            "Accuracy:\t 0.5190839694656488\n",
            "Recall:\t\t 0.5133333333333333\n",
            "Precision:\t 0.5923076923076923\n",
            "F1 Measure:\t 0.5499999999999999\n",
            "-----------------------------------------------------------\n",
            "\n",
            "-----------------------------------------------------------\n",
            "\n",
            "Classifier:\t Naive Bayes without Stop-Words\n",
            "\n",
            "Accuracy:\t 0.5190839694656488\n",
            "Recall:\t\t 0.5133333333333333\n",
            "Precision:\t 0.5923076923076923\n",
            "F1 Measure:\t 0.5499999999999999\n",
            "-----------------------------------------------------------\n",
            "\n",
            "-----------------------------------------------------------\n",
            "\n",
            "Classifier:\t Boolean Naive Bayes with Stop-Words\n",
            "\n",
            "Accuracy:\t 0.5534351145038168\n",
            "Recall:\t\t 0.6333333333333333\n",
            "Precision:\t 0.6050955414012739\n",
            "F1 Measure:\t 0.6188925081433224\n",
            "-----------------------------------------------------------\n",
            "\n",
            "-----------------------------------------------------------\n",
            "\n",
            "Classifier:\t Boolean Naive Bayes without Stop-Words\n",
            "\n",
            "Accuracy:\t 0.5572519083969466\n",
            "Recall:\t\t 0.5533333333333333\n",
            "Precision:\t 0.6287878787878788\n",
            "F1 Measure:\t 0.5886524822695035\n",
            "-----------------------------------------------------------\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aCNv2w2X1cmT"
      },
      "source": [
        "### **Report**\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "> **Procedure:**\n",
        "\n",
        "*   Read fake and real train data files in 2 seperate lists (fake, real) where each list item represents each file text. Similarly, all files were read.\n",
        "*   Read stop-words.txt in the form of list of tokens\n",
        "*   Removed duplicates words from each text and stored results in 2 new lists.\n",
        "*   Removed stop-words from each text and stored results in 2 new lists.\n",
        "*   Preprossed data and remove unnecassary words.\n",
        "*   Tokenized training data lists (fake, real).\n",
        "*   Made unigram model for each training data list (fake, real).\n",
        "*   Extracted vocabolary from all training data text.\n",
        "*   Developed generic Naive Bayes Model\n",
        "*   Evaluated performace\n",
        "\n",
        "\n",
        "> **Accuracy:**\n",
        "\n",
        "*   Naive Bayes With Stopwords : 51.9 %\n",
        "*   Naive Bayes Without Stopwords : 51.9 %\n",
        "*   Boolean Naive Bayes With Stopwords : 55.34 %\n",
        "*   Boolean Naive Bayes Without Stopwords : 55.72 %"
      ]
    }
  ]
}